{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT-word-embeddings.ipynb","provenance":[{"file_id":"1ZQvuAVwA3IjybezQOXnrXMGAnMyZRuPU","timestamp":1589078298273},{"file_id":"1FsBCkREOaDopLF3PIYUuQxLR8wRfjQY1","timestamp":1559844903389},{"file_id":"1f_snPs--PVYgZJwT3GwjxqVALFJ0T2-y","timestamp":1554843110227}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mIWPxWzbAHkt","colab_type":"text"},"source":["This notebook generates word embeddings using BERT. So, instead of passing sentences to BERT, words are passed. It is based on the following blog post: https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#1-loading-pre-trained-bert"]},{"cell_type":"code","metadata":{"id":"1RfUN_KolV-f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":386},"executionInfo":{"status":"ok","timestamp":1593480051630,"user_tz":-180,"elapsed":7952,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}},"outputId":"1fdf9711-6e5d-4ac0-b498-e097dee64c3b"},"source":["!pip install pytorch-pretrained-bert"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pytorch-pretrained-bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.5.1+cu101)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.9)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n","Requirement already satisfied: botocore<1.18.0,>=1.17.9 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.17.9)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->pytorch-pretrained-bert) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->pytorch-pretrained-bert) (0.15.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.9->boto3->pytorch-pretrained-bert) (1.12.0)\n","Installing collected packages: pytorch-pretrained-bert\n","Successfully installed pytorch-pretrained-bert-0.6.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lJEnBJ3gHTsQ","colab_type":"code","colab":{}},"source":["import torch\n","from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n","import numpy as np\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vr-67dIRC8y6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593480093104,"user_tz":-180,"elapsed":1389,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}},"outputId":"c8f5fdf6-6343-4775-bf74-1839e63a8d8e"},"source":["torch.cuda.is_available()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"Kp3bScOVSkeO","colab_type":"text"},"source":["# Methods"]},{"cell_type":"code","metadata":{"id":"A9VH1E8xOT-n","colab_type":"code","colab":{}},"source":["def read_data(data_directory, data_file, tokenizer, old_dictionary=None):\n","  \"\"\"\n","  This method reads data sentences and transforms them to accepted format by BERT's tokenizer. Dictionary is\n","    used to collect uniqe words of data. Each value in the dictionary is set to 0, \n","    and it is not used.\n","\n","  This method is used once to collect all uniqe words for a given dataset. Then, the words are written to a vocab file. When one \n","    wants to generate embeddings, they do not have to run this method. They just need to read the vocab file corresponding to the dataset.  \n","\n","  If vocabs (uniqe words) of several datasets are to be generated, then one needs to set \"old_dictionary\" to \"None\" for the first dataset. After that,\n","    they need to set \"old_dictionary\" to dictionary returned by the previous call to the method. See Read Data section for an example.\n","  \"\"\"\n","\n","  print(\"Processing data in {}\".format(data_directory + data_file))\n","\n","  dictionary = old_dictionary if old_dictionary else {}\n","  counter = 0\n","\n","  with open(data_directory + data_file, mode=\"r\") as data_file:\n","    start_time = time.time()\n","    for line in data_file:\n","        sentence = \"[CLS] \" + line + \" [SEP]\"\n","        tokenized_sentence = tokenizer.tokenize(sentence)\n","\n","        for element in tokenized_sentence:\n","          dictionary[element] = 0\n","\n","        counter += 1\n","        if counter % 100000 == 0:\n","            print(\"Done with line %d\" % counter)\n","            print(\"Elapsed time in minutes is:\", round((time.time() - start_time)/60, 4))\n","\n","  avg_time = (time.time() - start_time) * (100000 / counter) * (1 / 60)\n","  print(\"Process is done, and average time (in minutes) needed for every 100000 lines is\", round(avg_time, 4))\n","  return dictionary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0OdszmEX9Xt","colab_type":"code","colab":{}},"source":["def write_vocab(dictionary, vocab_directory, vocab_file=\"vocab.txt\"):\n","  \"\"\"\n","  This method writes all uniqe words to a vocab file.\n","  \"\"\"\n","  with open(vocab_directory + vocab_file, 'w') as f:\n","    for key in dictionary.keys():\n","        f.write(key + \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FdXbNH8_X-xs","colab_type":"code","colab":{}},"source":["def read_vocab(vocab_directory, vocab_file=\"vocab.txt\"):\n","  dictionary = {}\n","  with open(vocab_directory + vocab_file, 'r') as f:\n","    for line in f:\n","      dictionary[line.strip()] = 0\n","\n","  return dictionary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBr2tC5U63IH","colab_type":"code","colab":{}},"source":["def get_embeddings(dictionary, tokenizer, model, mode=\"first\"):\n","  \"\"\"\n","  This method extracts embeddings from BERT's layers as specified by \"mode\"\n","  \"\"\"\n","\n","  counter = 0\n","  start_time = time.time()\n","  for key in dictionary:\n","    \n","    counter += 1\n","    if (counter%1000 == 0):\n","      print(\"Processing element: \", counter)\n","      print(\"Elapsed time in minutes is:\", round((time.time() - start_time)/60, 4))\n","    \n","    list_ = [key]\n","    segments_ids = [1] * len(list_)\n","\n","    id_tokens = tokenizer.convert_tokens_to_ids(list_)\n","\n","    # Convert inputs to PyTorch tensors\n","    tokens_tensor = torch.tensor([id_tokens], device=torch.device(\"cuda:0\"))\n","    segments_tensors = torch.tensor([segments_ids], device=torch.device(\"cuda:0\"))\n","\n","    with torch.no_grad():\n","      encoded_layers, _ = model(tokens_tensor, segments_tensors)\n","\n","    token_embeddings = torch.stack(encoded_layers, dim=0)\n","    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n","\n","    if (mode==\"first\"):\n","      dictionary[key] = token_embeddings[0][0].cpu().numpy()\n","    elif (mode==\"mid\"):\n","      dictionary[key] = np.mean(token_embeddings[4:6, 0, :].cpu().numpy(), axis=0)\n","    elif (mode==\"last\"):\n","      dictionary[key] = token_embeddings[-1][0].cpu().numpy()\n","    elif (mode==\"avg_1st_2nd\"):\n","      dictionary[key] = np.mean(token_embeddings[0:2, 0, :].cpu().numpy(), axis=0)\n","    elif (mode==\"concat_1st_2nd\"):\n","      dictionary[key] = np.concatenate((token_embeddings[0:2, 0, :].cpu().numpy()), axis=0)\n","    elif (mode==\"avg\"):\n","      dictionary[key] = np.mean(token_embeddings[-4:, 0, :].cpu().numpy(), axis=0)\n","\n","  avg_time = (time.time() - start_time) * (1000 / counter) * (1 / 60)\n","  print(\"Process is done, and average time (in minutes) needed for every 1000 elements is\", round(avg_time, 4))\n","  return dictionary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTd2oWef6v9K","colab_type":"code","colab":{}},"source":["def write_embeddings(dictionary, embeddings_directory, embeddings_file=\"embeddingsFile.txt\"):\n","\n","  with open(embeddings_directory + embeddings_file, mode='w+') as data_file:\n","    counter = 0\n","    for key in dictionary:\n","        counter += 1\n","        data_file.write(key + ' ')\n","        np.savetxt(data_file, dictionary[key], delimiter=' ', newline=' ')\n","        data_file.write('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VOuqVZPr77KI","colab_type":"code","colab":{}},"source":["def read_embeddings(embeddings_directory, embeddings_file):\n","\n","  embedding_matrix = {}\n","  with open(embeddings_directory + embeddings_file, mode='r') as data_file:\n","    counter = 0\n","    for line in data_file:\n","      counter += 1\n","      element = line.split()\n","      embedding_matrix[element[0]] = np.asarray([float(x) for x in element[1:]], dtype=np.float32)\n","\n","  return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ckmmYa0891xO","colab_type":"text"},"source":["# Define Model"]},{"cell_type":"code","metadata":{"id":"UekmCBtj6Whn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593480308596,"user_tz":-180,"elapsed":52063,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}},"outputId":"a7bc2e48-c738-4edc-b844-5d4a3b4569a6"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","model = BertModel.from_pretrained('bert-base-uncased')\n","model.to(device=torch.device(\"cuda:0\"))\n","model.eval()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 231508/231508 [00:00<00:00, 352234.20B/s]\n","100%|██████████| 407873900/407873900 [00:30<00:00, 13263858.56B/s]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): BertLayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): BertLayerNorm()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"5qnAdjrc9ypb","colab_type":"text"},"source":["# Read Data"]},{"cell_type":"markdown","metadata":{"id":"BdeDYgd5_aHU","colab_type":"text"},"source":["## Ready vocab file"]},{"cell_type":"markdown","metadata":{"id":"qhS2kit0_fU8","colab_type":"text"},"source":["If there is a vocab file, run the next cell and ignore the 5 cells after it."]},{"cell_type":"code","metadata":{"id":"Tv74UNd0_YEO","colab_type":"code","colab":{}},"source":["vocab_directory = \"./\"\n","vocab = read_vocab(vocab_directory)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lk3Bt-U5_oqu","colab_type":"text"},"source":["Ignore if not needed."]},{"cell_type":"code","metadata":{"id":"FBhiUzXDT5qj","colab_type":"code","colab":{}},"source":["data_directory = \"./\"\n","dictionary = read_data(data_directory, \"CBT-NE.txt\", tokenizer, old_dictionary=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aziCq7UnUzTD","colab_type":"code","colab":{}},"source":["dictionary = read_data(data_directory, \"CBT-CN.txt\", tokenizer, old_dictionary = dictionary)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OURYS_bzYPch","colab_type":"code","colab":{}},"source":["vocab_directory = \"./\"\n","write_vocab(dictionary, vocab_directory)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xZ1PVFFK9-RN","colab_type":"text"},"source":["# Get Embeddings"]},{"cell_type":"code","metadata":{"id":"RCu0yf-6VEbu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":907},"executionInfo":{"status":"ok","timestamp":1593480731891,"user_tz":-180,"elapsed":311375,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}},"outputId":"cfd974bb-be39-4c25-d7ea-160dd3ac1c36"},"source":["vocab = get_embeddings(vocab, tokenizer, model, mode=\"avg_1st_2nd\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processing element:  1000\n","Elapsed time in minutes is: 0.1982\n","Processing element:  2000\n","Elapsed time in minutes is: 0.3911\n","Processing element:  3000\n","Elapsed time in minutes is: 0.5831\n","Processing element:  4000\n","Elapsed time in minutes is: 0.7749\n","Processing element:  5000\n","Elapsed time in minutes is: 0.9693\n","Processing element:  6000\n","Elapsed time in minutes is: 1.1615\n","Processing element:  7000\n","Elapsed time in minutes is: 1.357\n","Processing element:  8000\n","Elapsed time in minutes is: 1.549\n","Processing element:  9000\n","Elapsed time in minutes is: 1.7433\n","Processing element:  10000\n","Elapsed time in minutes is: 1.9431\n","Processing element:  11000\n","Elapsed time in minutes is: 2.1387\n","Processing element:  12000\n","Elapsed time in minutes is: 2.3344\n","Processing element:  13000\n","Elapsed time in minutes is: 2.5313\n","Processing element:  14000\n","Elapsed time in minutes is: 2.7236\n","Processing element:  15000\n","Elapsed time in minutes is: 2.9167\n","Processing element:  16000\n","Elapsed time in minutes is: 3.1128\n","Processing element:  17000\n","Elapsed time in minutes is: 3.3044\n","Processing element:  18000\n","Elapsed time in minutes is: 3.5011\n","Processing element:  19000\n","Elapsed time in minutes is: 3.6963\n","Processing element:  20000\n","Elapsed time in minutes is: 3.8868\n","Processing element:  21000\n","Elapsed time in minutes is: 4.0794\n","Processing element:  22000\n","Elapsed time in minutes is: 4.2707\n","Processing element:  23000\n","Elapsed time in minutes is: 4.4613\n","Processing element:  24000\n","Elapsed time in minutes is: 4.6546\n","Processing element:  25000\n","Elapsed time in minutes is: 4.8514\n","Processing element:  26000\n","Elapsed time in minutes is: 5.045\n","Process is done, and average time (in minutes) needed for every 1000 elements is 0.194\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nLTH-hJY-JXU","colab_type":"text"},"source":["# Write Embeddings"]},{"cell_type":"code","metadata":{"id":"F2BLffuY4sul","colab_type":"code","colab":{}},"source":["embeddings_directory = \"/content/drive/My Drive/192/Thesis/embeddings/\"\n","write_embeddings(vocab, embeddings_directory, embeddings_file=\"embeddingsAvg_1st_2nd.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jiWjnEly-XIp","colab_type":"text"},"source":["# Read Embeddings"]},{"cell_type":"code","metadata":{"id":"5iZM2ouznLMH","colab_type":"code","colab":{}},"source":["embeddings_directory = \"/content/drive/My Drive/192/Thesis/embeddings/\"\n","embedding_matrix = read_embeddings(embeddings_directory, \"embeddingsConcat_1st_2nd.txt\")"],"execution_count":null,"outputs":[]}]}