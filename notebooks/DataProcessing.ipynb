{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataProcessing.ipynb","provenance":[],"collapsed_sections":["N9kNYZ9WYdsM"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TfoN628P2vlI"},"source":["This notebook performs the following:\n","1. Convert data from text to ID.\n","2. Convert CNN and Daily Mail data into CBT format.\n","3. Some analysis done on data."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EgSixwI4FRuP"},"source":["# Imports"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"335cNcUDFDPZ","colab":{}},"source":["import os\n","import re\n","import nltk\n","from collections import Counter\n","from functools import reduce\n","from tensorflow.python.platform import gfile\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import random\n","import glob"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xm-tEC2-F94Q"},"source":["# Global Parameters"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9V3kz0lhFvz-","colab":{}},"source":["\"\"\"\n","Tags to be added to vocabulary\n","Padding, Start of Sentence, Snd of Sentence, Snknown, and End of Question\n","\"\"\"\n","PAD = \"PAD\"\n","SoS = \"SoS\"\n","EoS = \"EoS\"\n","UNK = \"UNK\"\n","EoQ = \"EoQ\"\n","\n","tags = [PAD, SoS, EoS, UNK, EoQ]\n","\n","# Tag IDs\n","PAD_ID = 0\n","SoS_ID = 1\n","EoS_ID = 2\n","UNK_ID = 3\n","EoQ_ID = 4\n","\n","# Number of words to keep\n","vocab_size = 100000\n","\n","\n","\"\"\"\n","The following is for converting CNN and Daily Mail data into CBT style. The following tags are POS tags that\n","    are used to collect true answers from documents. Corresponding code could be found at the end of this \n","    notebook under the section Converting CNN and Daily Mail Data.\n","NN = noun\n","NNS = noun plural\n","NNP = proper noun, singular\n","NNPS = proper noun, plural\n","VB = verb\n","VBG = verb gerund\n","VBD = verb past tense\n","VBN = verb past participle\n","\n","\"\"\"\n","\n","included_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBG', 'VBD', 'VBN']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bY4evWXpFy_o"},"source":["# Methods"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lq-0Jbo86b0z"},"source":["Methods in this section are based on methods in https://github.com/cairoHy/attention-sum-reader/blob/master/data_utils.py. The methods are used to convert words to IDs."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"e1zSbFb6IoiJ"},"source":["## Tokenizer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6CMLnDdUF1gm","colab":{}},"source":["def default_tokenizer(sentence):\n","\n","    \"\"\"\n","    A regular expression (RE) to matche a digit follwed by any number of digits\n","    This is needed to remove line numbers at the start of each sentence\n","    \"\"\"\n","    digit_RE = re.compile(r\"\\d+\")\n","\n","    # Replace line number with the empty string\n","    sentence = digit_RE.sub(\"\", sentence)\n","\n","    \"\"\"\n","    The following line splits a string at '|', and return it without '|'\n","    It is needed for the query sentence\n","    \"\"\"\n","    sentence = \" \".join(sentence.split(\"|\"))\n","\n","    \"\"\"\n","    'nltk.word_tokenize(sentence.lower())' returns a list with each entry is a word (or a char) in the original string.\n","    It ignores spaces and '\\n'\n","    \"\"\"\n","    return nltk.word_tokenize(sentence.lower())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m8yeRLkvKo0B"},"source":["## Vocabulary"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Jvl8zCDlKewa","colab":{}},"source":["def gen_vocab(data_file, tokenizer=default_tokenizer, old_counter=None):\n","  \"\"\"\n","  This method reads input data and returns a counter object with entries as {word : its frequency}\n","\n","  Parameters:\n","    'data_file': Directory to CBT data\n","    'tokenizer': Tokenizer to tokenize sentences\n","    'old_counter': A counter object with entries as {word : its frequency}\n","  \"\"\"\n","\n","  print(\"Creating word_dict from data %s\" % data_file)\n","\n","  # Check if a counter object is passed\n","  word_counter = old_counter if old_counter else Counter()\n","  counter = 0\n","\n","  with gfile.GFile(data_file) as f:\n","    for line in f:\n","      counter += 1\n","\n","      # 'str.rstrip('\\n')' removes mew line char\n","      tokens = tokenizer(line.rstrip('\\n'))\n","\n","      # Update Counter\n","      word_counter.update(tokens)\n","\n","      if counter % 100000 == 0:\n","        print(\"Done processing line %d.\" % counter)\n","\n","  # Some statistics\n","  total_words = sum(word_counter.values())\n","  distinct_words = len(list(word_counter))\n","\n","  print(\"Some statistics:\")\n","  print(\"Total words: \" + str(total_words))\n","  print(\"Total distinct words: \" + str(distinct_words))\n","\n","  return word_counter\n","\n","\n","def save_vocab(word_counter, vocab_file, vocab_size=None):\n","  \"\"\"\n","  This method processes Counter object and generates a file with 'vocab_size' words.\n","\n","  Parameters:\n","    'word_counter': Counter object\n","    'vocab_file': A file to write the vocab to\n","    'vocab_size': The maximum nember of words to keep\n","  \"\"\"\n","\n","  with gfile.GFile(vocab_file, \"w\") as f:\n","    for word in tags:\n","      f.write(word + \"\\n\")\n","\n","    \"\"\"\n","    'word_counter.most_common(vocab_size)' return the 'vocab_size' most common words.\n","    'map(fun, iter)' ==> fun = 'lambda x: x[0]', iter = 'word_counter.most_common(vocab_size)'\n","    'x' = (word, freq)\n","    \"\"\"\n","    for word in list(map(lambda x: x[0], word_counter.most_common(vocab_size))):\n","      f.write(word + \"\\n\")\n","\n","\n","def load_vocab(vocab_file):\n","  \"\"\"\n","  This method loads 'vocab_file'. It returns a 'word_dict' with entries as {word : its ID}\n","\n","  Parameters:\n","    'vocab_file': Path to vocab file\n","  \"\"\"\n","\n","  if not gfile.Exists(vocab_file):\n","    raise ValueError(\"Vocabulary file %s not found.\", vocab_file)\n","\n","  word_dict = {}\n","  word_id = 0\n","\n","  with gfile.GFile(vocab_file, \"r\") as f:\n","    for line in f:\n","\n","      # Line has a single word with trailing new line char which needs to be removed\n","      word_dict.update({line.strip(): word_id})\n","      word_id += 1\n","\n","  return word_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qCN1nc99RADh"},"source":["## Main Methods"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kP-HdAbCQzTq","colab":{}},"source":["def sentence_to_token_ids(sentence, word_dict, tokenizer=default_tokenizer):\n","  \"\"\"\n","  This method translates words in a sentence into the corresponding IDs\n","  Example：\n","    sentence: [\"I\", \"have\", \"a\", \"dog\"]\n","    word_list：{\"I\": 1, \"have\": 2, \"a\": 4, \"dog\": 7\"}\n","    return: [1, 2, 4, 7]\n","\n","  Parameters:\n","    'sentence': input string\n","    'word_dict': Word -> ID mapping dictionary\n","    'tokenizer': Tokenizer to tokenize sentences\n","\n","    Returns: List of IDs\n","  \"\"\"\n","\n","  \"\"\"\n","  'dictionary.get(keyname, value)', 'value' is the value to return if \n","    'keyname' does not exist\n","  \"\"\"\n","  return [word_dict.get(token, UNK_ID) for token in tokenizer(sentence)]\n","\n","\n","def data_to_token_ids(data_file, target_file, vocab_file):\n","  \"\"\"\n","  This method generates a file out of the input data such that each word is replaced \n","    by its ID\n","\n","  Each CBT example has 22 lines as:\n","    First 20 lines: Context with number of lines (e.g., 1 to 20)\n","    Line 21: Question with line number (e.g., 21) \\t true answer \\t \\t candidate answer 1 | candidate answer 2 | ... | candidate answer 10\n","    Line 22: Blank\n","\n","  Parameters:\n","    'data_file': Directory to CBT data file\n","    'target_file': A file to write IDs to\n","    'vocab_file': vocab file\n","  \"\"\"\n","\n","  if gfile.Exists(target_file):\n","      return\n","\n","  print(\"Tokenizing data in {}\".format(data_file))\n","\n","  word_dict = load_vocab(vocab_file)\n","  counter = 0\n","    \n","  document = ''\n","  \n","  with gfile.GFile(data_file, mode=\"r\") as data_file:\n","    with gfile.GFile(target_file, mode=\"w\") as tokens_file:\n","      for line in data_file:\n","        counter += 1\n","        if counter % 100000 == 0:\n","          print(\"Tokenizing line %d\" % counter)\n","\n","        if counter % 22 == 21:\n","          \"\"\"\n","          line.split(\"\\t\")[0] = query\n","          line.split(\"\\t\")[1] = true answer\n","          line.split(\"\\t\")[2] = empty string\n","          line.split(\"\\t\")[3] = candidate answers\n","          \"\"\"\n","          q, true_answer, _, CA = line.split(\"\\t\")\n","\n","          token_ids_q = sentence_to_token_ids(q, word_dict)\n","          token_ids_CA = [word_dict.get(a.lower(), UNK_ID) for a in CA.rstrip(\"\\n\").split(\"|\")]\n","          query = \" \".join([str(tok) for tok in token_ids_q]) + \"\\t\" \\\n","                            + str(word_dict.get(true_answer.lower(), UNK_ID)) + \"\\t\" \\\n","                            + \"|\".join([str(tok) for tok in token_ids_CA]) + \"\\n\"\n","          \n","          tokens_file.write(document + query)\n","          document = ''\n","          \n","        elif counter % 22 < 21:\n","          token_ids = sentence_to_token_ids(line, word_dict)\n","          document = document + \" \".join([str(tok) for tok in token_ids]) + \"\\n\"\n","          #tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")\n","\n","def prepare_data(data_dir, train_file, valid_file, test_file, vocab_size, output_dir):\n","  \"\"\"\n","  This method takes paths to all data files and generates corresponding ID files\n","  \"\"\"\n","\n","  if not gfile.Exists(os.path.join(data_dir, output_dir)):\n","    os.mkdir(os.path.join(data_dir, output_dir))\n","\n","  os_train_file = os.path.join(data_dir, train_file + '.txt')\n","  os_valid_file = os.path.join(data_dir, valid_file + '.txt')\n","  os_test_file = os.path.join(data_dir, test_file + '.txt')\n","\n","  id_train_file = os.path.join(data_dir, output_dir, train_file + \".%d.id.txt\" % vocab_size)\n","  id_valid_file = os.path.join(data_dir, output_dir, valid_file + \".%d.id.txt\" % vocab_size)\n","  id_test_file = os.path.join(data_dir, output_dir, test_file + \".%d.id.txt\" % vocab_size)\n","  vocab_file = os.path.join(data_dir, output_dir, \"vocab.%d.txt\" % vocab_size)\n","  \n","  if not gfile.Exists(vocab_file):\n","    word_counter = gen_vocab(os_train_file)\n","    word_counter = gen_vocab(os_valid_file, old_counter=word_counter)\n","    word_counter = gen_vocab(os_test_file, old_counter=word_counter)\n","    save_vocab(word_counter, vocab_file, vocab_size)\n","\n","  # Create train, valid, and test files represented by IDs\n","  data_to_token_ids(os_train_file, id_train_file, vocab_file)\n","  data_to_token_ids(os_valid_file, id_valid_file, vocab_file)\n","  data_to_token_ids(os_test_file, id_test_file, vocab_file)\n","\n","  return vocab_file, id_train_file, id_valid_file, id_test_file"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"N9kNYZ9WYdsM"},"source":["# Prepare Data"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fzkSYHZElAcu"},"source":["## CBT"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ccj0h3VkU3sC","colab":{}},"source":["\"\"\"\n","'cloze_types' is a list to hold types of missing words (e.g., Named Entity (NE), Common Noun (CN), and so on..).\n","  The list is needed to biuld paths to corresponding files. Instead of building 4 sets of paths, it is better\n","  to build one set and only change the part that identifies the missimg word's type. So, the same set of paths \n","  could be used to access all data files.\n","\n","'output_dir' is the directory to store ID files. The same directory name could be used for all data types (e.g., NE, CN, and so on..)\n","  because 'output_dir' is a sub-directory in each data type's folder.\n","\"\"\"\n","\n","cloze_types = [\"CN\", \"NE\", \"V\", \"P\"]\n","output_dir = \"processedData\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"amQqgtCQYeuo","colab":{}},"source":["for cloze_type in cloze_types:\n","    # Directories to CBT data\n","    data_dir = \"./data/\" + cloze_type + \"_data/\"\n","    train_file = \"cbtest_\" + cloze_type + \"_train\"\n","    valid_file = \"cbtest_\" + cloze_type + \"_valid_2000ex\"\n","    test_file = \"cbtest_\" + cloze_type + \"_test_2500ex\"\n","\n","    vocab_file, id_train_file, id_valid_file, id_test_file = prepare_data(\n","          data_dir, train_file, valid_file, test_file, vocab_size, output_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hDJ56-cOlAc0"},"source":["## CNN and Daily Mail"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ETlTj1Vk4nje","colab":{}},"source":["\"\"\"\n","'output_dir' is the directory to store ID files. The same directory name could be used for all data types (e.g., NE, CN, and so on..)\n","  because 'output_dir' is a sub-directory in each data type's folder.\n","\"\"\"\n","\n","dataset_name = [\"CNN\", \"DM\"]\n","output_dir = \"processedData\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Xjwqn3OplAc5","colab":{},"outputId":"53a67185-2373-438d-e688-2bd091b3ed01"},"source":["for name in dataset_name:\n","    # Directories to data\n","    data_dir = \"./data/\" + name + \"_data/\"\n","    train_file = name + \"_train\"\n","    valid_file = name + \"_valid_2000ex\"\n","    test_file = name + \"_test_2500ex\"\n","\n","    vocab_file, id_train_file, id_valid_file, id_test_file = prepare_data(\n","          data_dir, train_file, valid_file, test_file, vocab_size, output_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Creating word_dict from data ./data/CNN_data/CNN_train.txt\n","Done processing line 100000.\n","Done processing line 200000.\n","Done processing line 300000.\n","Done processing line 400000.\n","Done processing line 500000.\n","Done processing line 600000.\n","Done processing line 700000.\n","Done processing line 800000.\n","Done processing line 900000.\n","Done processing line 1000000.\n","Done processing line 1100000.\n","Done processing line 1200000.\n","Done processing line 1300000.\n","Some statistics:\n","Total words: 44884418\n","Total distinct words: 252787\n","Creating word_dict from data ./data/CNN_data/CNN_valid_2000ex.txt\n","Some statistics:\n","Total words: 45889828\n","Total distinct words: 255469\n","Creating word_dict from data ./data/CNN_data/CNN_test_2500ex.txt\n","Some statistics:\n","Total words: 47179397\n","Total distinct words: 259034\n","Tokenizing data in ./data/CNN_data/CNN_train.txt\n","Tokenizing line 100000\n","Tokenizing line 200000\n","Tokenizing line 300000\n","Tokenizing line 400000\n","Tokenizing line 500000\n","Tokenizing line 600000\n","Tokenizing line 700000\n","Tokenizing line 800000\n","Tokenizing line 900000\n","Tokenizing line 1000000\n","Tokenizing line 1100000\n","Tokenizing line 1200000\n","Tokenizing line 1300000\n","Tokenizing data in ./data/CNN_data/CNN_valid_2000ex.txt\n","Tokenizing data in ./data/CNN_data/CNN_test_2500ex.txt\n","Creating word_dict from data ./data/DM_data/DM_train.txt\n","Done processing line 100000.\n","Done processing line 200000.\n","Done processing line 300000.\n","Done processing line 400000.\n","Done processing line 500000.\n","Done processing line 600000.\n","Done processing line 700000.\n","Done processing line 800000.\n","Done processing line 900000.\n","Done processing line 1000000.\n","Done processing line 1100000.\n","Done processing line 1200000.\n","Done processing line 1300000.\n","Done processing line 1400000.\n","Done processing line 1500000.\n","Done processing line 1600000.\n","Done processing line 1700000.\n","Done processing line 1800000.\n","Done processing line 1900000.\n","Some statistics:\n","Total words: 38914441\n","Total distinct words: 266804\n","Creating word_dict from data ./data/DM_data/DM_valid_2000ex.txt\n","Some statistics:\n","Total words: 39689795\n","Total distinct words: 269593\n","Creating word_dict from data ./data/DM_data/DM_test_2500ex.txt\n","Some statistics:\n","Total words: 40672463\n","Total distinct words: 273186\n","Tokenizing data in ./data/DM_data/DM_train.txt\n","Tokenizing line 100000\n","Tokenizing line 200000\n","Tokenizing line 300000\n","Tokenizing line 400000\n","Tokenizing line 500000\n","Tokenizing line 600000\n","Tokenizing line 700000\n","Tokenizing line 800000\n","Tokenizing line 900000\n","Tokenizing line 1000000\n","Tokenizing line 1100000\n","Tokenizing line 1200000\n","Tokenizing line 1300000\n","Tokenizing line 1400000\n","Tokenizing line 1500000\n","Tokenizing line 1600000\n","Tokenizing line 1700000\n","Tokenizing line 1800000\n","Tokenizing line 1900000\n","Tokenizing data in ./data/DM_data/DM_valid_2000ex.txt\n","Tokenizing data in ./data/DM_data/DM_test_2500ex.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ea20E2QelAc9"},"source":["# Converting CNN and Daily Mail Data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"USCaRymRlAc9","colab":{}},"source":["# A method to get all file names of CNN and Daily Mail data\n","def getFileNames (extension):\n","    return [file for file in glob.glob(extension)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YzJ4cMPDlAc_","colab":{}},"source":["# A method to convert CNN and Daily Mail data into CBT style by processing a file at a time\n","def writeData(input_files, output_file):\n","    \"\"\"\n","    This method converts CNN and Daily Mail data into CBT style by processing a file at a time. It returns the \n","        number of written files.\n","\n","  Parameters:\n","    'input_files': A list of directories to CNN or Daily Mail data.\n","    'output_file': A file to write converted data to.\n","    'valid_query_length': minimum length of a query. If a query has less than 'valid_query_length' words,\n","        then the context is ignored.\n","    \"\"\"\n","    # Keep track of line numbers\n","    line_number = 1\n","    \n","    # String to store context\n","    document = ''\n","\n","    \"\"\"\n","    A counter to keep track of number of converted files. It is needed because some files \n","        have contexts of less than 20 lines. Such files are ignored.\n","    \"\"\"\n","    converted_files = 0\n","    \n","    \"\"\"\n","    A counter to keep track of number of processed files.\n","        \n","    \"\"\"\n","    processed_files = 0\n","    \n","    for file in input_files:\n","        \n","        # Reset line counter\n","        line_number = 1\n","\n","        # Reset 'document'\n","        document = ''\n","                    \n","        processed_files += 1\n","        if processed_files % 1000 == 0:\n","            print(\"Processed files so far is \", processed_files)\n","            \n","        with open(file, mode=\"r\") as f:\n","            with open(output_file, mode=\"a+\") as w:\n","            \n","                for line in f:\n","\n","                    # Collect context of 20 lines, and check line is not empty and does not have '@highlight'\n","                    if line_number < 21 and line != '\\n' and not (\"@highlight\" in line):\n","                            \n","                        # Add '\\n' to 'line' if it does not have it\n","                        line = line if '\\n' in line else line + '\\n'\n","                        \n","                        # Build context line by line\n","                        document = document + str(line_number) + ' ' + line\n","                        \n","                        line_number += 1\n","\n","                    # line #21 is query line\n","                    elif line_number == 21 and line != '\\n' and not (\"@highlight\" in line):\n","                        \n","                        # Remove non-alphanumeric values from 'line'\n","                        line = re.sub('[^A-Za-z0-9]+', ' ', line)\n","                        \n","                        # Remove single characters from 'line'\n","                        line =  re.sub(r\"\\b[a-zA-Z]\\b\", \"\", line)\n","                        \n","                        # Tokenize line, POS tag it, and shuffle tags \n","                        tokenized_line = word_tokenize(line)\n","                        tags = nltk.pos_tag(tokenized_line)\n","                        random.shuffle(tags)\n","\n","                        \"\"\"\n","                        Get first word in 'tags' with accepted tag. If there is no such word, discard \n","                          the whole document.\n","                        \"\"\"\n","\n","                        selected_word = None\n","                        for word, tag in tags:\n","                            #selected_word = word\n","                            if tag in included_tags:\n","                                selected_word = word\n","                                break\n","\n","                        if (selected_word):\n","                            \n","                            # Construct a temporary document without 'selected_word'\n","                            document_tmp = re.sub(selected_word, '', document)\n","\n","                            # Check if population (document_tmp) has more than 9 uniqe words\n","                            if (len(document_tmp.split()) > 9):\n","                                    \n","                                # Randomly sample 9 candidatie answers from 'document_tmp'\n","                                candidate_answers = random.sample(document_tmp.split(), 9)\n","\n","                                # Append 'selected_word' to 'candidate_answers' and shuffle it\n","                                candidate_answers.append(selected_word)\n","                                random.shuffle(candidate_answers)\n","\n","                                # Build query line\n","                                query = str(line_number) + ' ' + line.rstrip().replace(selected_word, 'XXXXX', 1)\n","\n","                                query_line = query + '\\t' + selected_word + '\\t' + '' + '\\t' + '|'.join(candidate_answers)\n","\n","                                # write 'document' and 'query', and add empty line after them\n","                                w.write(document + query_line + '\\n\\n')\n","\n","                                # Advance converted files counter\n","                                converted_files += 1\n","                                \n","                                # Stop reading data\n","                                break\n","                        \n","                        \n","                        \n","    print(\"Number of converted files is \", converted_files)\n","    return converted_files"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EzsicZxJlAdC"},"source":["## CNN"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rMmL-6I2lAdD","colab":{}},"source":["cnn_files = getFileNames(\"./cnn_stories/cnn/stories/*.story\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bTxY6VAxlAdI","colab":{},"outputId":"72f02e0e-658d-4d1f-da3d-b459eb6f6782"},"source":["print(\"Number of CNN data files is\", len(cnn_files))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of CNN data files is 92579\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8tfwZrL2lAdK","colab":{}},"source":["# Split into train, validation, and test set\n","\n","# Sizes of sets\n","test_size = 2500\n","val_size = 2000\n","train_size = len(cnn_files) - (test_size + val_size)\n","\n","random.shuffle(cnn_files)\n","\n","train_files = cnn_files[0 : train_size]\n","val_files = cnn_files[train_size : (train_size + val_size)]\n","test_files = cnn_files[(train_size + val_size) : (train_size + val_size + train_size)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"k9i5N6P9lAdN","colab":{},"outputId":"a654eb6e-a8f5-4992-9458-2aa39cc75bc9"},"source":["print(\"Number of CNN train data files is\", len(train_files))\n","print(\"Number of CNN validation data files is\", len(val_files))\n","print(\"Number of CNN test data files is\", len(test_files))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of CNN train data files is 88079\n","Number of CNN validation data files is 2000\n","Number of CNN test data files is 2500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CIo8_X83lAdQ","scrolled":false,"colab":{},"outputId":"6387a92c-2fc4-4a47-ceb5-ff3c060bce28"},"source":["CNN_data_directory = './cnn_stories/'\n","total_train_data = writeData(train_files, CNN_data_directory + 'CNN_train6.txt')\n","total_val_data = writeData(val_files, CNN_data_directory + 'CNN_valid_2000ex6.txt')\n","total_test_data = writeData(test_files, CNN_data_directory + 'CNN_test_2500ex6.txt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processed files so far is  1000\n","Processed files so far is  2000\n","Processed files so far is  3000\n","Processed files so far is  4000\n","Processed files so far is  5000\n","Processed files so far is  6000\n","Processed files so far is  7000\n","Processed files so far is  8000\n","Processed files so far is  9000\n","Processed files so far is  10000\n","Processed files so far is  11000\n","Processed files so far is  12000\n","Processed files so far is  13000\n","Processed files so far is  14000\n","Processed files so far is  15000\n","Processed files so far is  16000\n","Processed files so far is  17000\n","Processed files so far is  18000\n","Processed files so far is  19000\n","Processed files so far is  20000\n","Processed files so far is  21000\n","Processed files so far is  22000\n","Processed files so far is  23000\n","Processed files so far is  24000\n","Processed files so far is  25000\n","Processed files so far is  26000\n","Processed files so far is  27000\n","Processed files so far is  28000\n","Processed files so far is  29000\n","Processed files so far is  30000\n","Processed files so far is  31000\n","Processed files so far is  32000\n","Processed files so far is  33000\n","Processed files so far is  34000\n","Processed files so far is  35000\n","Processed files so far is  36000\n","Processed files so far is  37000\n","Processed files so far is  38000\n","Processed files so far is  39000\n","Processed files so far is  40000\n","Processed files so far is  41000\n","Processed files so far is  42000\n","Processed files so far is  43000\n","Processed files so far is  44000\n","Processed files so far is  45000\n","Processed files so far is  46000\n","Processed files so far is  47000\n","Processed files so far is  48000\n","Processed files so far is  49000\n","Processed files so far is  50000\n","Processed files so far is  51000\n","Processed files so far is  52000\n","Processed files so far is  53000\n","Processed files so far is  54000\n","Processed files so far is  55000\n","Processed files so far is  56000\n","Processed files so far is  57000\n","Processed files so far is  58000\n","Processed files so far is  59000\n","Processed files so far is  60000\n","Processed files so far is  61000\n","Processed files so far is  62000\n","Processed files so far is  63000\n","Processed files so far is  64000\n","Processed files so far is  65000\n","Processed files so far is  66000\n","Processed files so far is  67000\n","Processed files so far is  68000\n","Processed files so far is  69000\n","Processed files so far is  70000\n","Processed files so far is  71000\n","Processed files so far is  72000\n","Processed files so far is  73000\n","Processed files so far is  74000\n","Processed files so far is  75000\n","Processed files so far is  76000\n","Processed files so far is  77000\n","Processed files so far is  78000\n","Processed files so far is  79000\n","Processed files so far is  80000\n","Processed files so far is  81000\n","Processed files so far is  82000\n","Processed files so far is  83000\n","Processed files so far is  84000\n","Processed files so far is  85000\n","Processed files so far is  86000\n","Processed files so far is  87000\n","Processed files so far is  88000\n","Number of converted files is  50862\n","Processed files so far is  1000\n","Processed files so far is  2000\n","Number of converted files is  1216\n","Processed files so far is  1000\n","Processed files so far is  2000\n","Number of converted files is  1461\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Sb0YDfSglAdX"},"source":["## Daily Mail"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NrbWek6_lAdX","colab":{}},"source":["DM_files = getFileNames(\"./dailymail_stories/dailymail/stories/*.story\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xKgoJdaylAda","colab":{},"outputId":"aa160147-75db-48ce-fd8f-ec400535b7ad"},"source":["print(\"Number of Daily Mail data files is\", len(DM_files))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of Daily Mail data files is 219506\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_OYQKRjVlAdc","colab":{}},"source":["# Split into train, validation, and test set\n","\n","# Sizes of sets\n","test_size = 2500\n","val_size = 2000\n","train_size = len(DM_files) - (test_size + val_size)\n","\n","random.shuffle(DM_files)\n","\n","train_files = DM_files[0 : train_size]\n","val_files = DM_files[train_size : (train_size + val_size)]\n","test_files = DM_files[(train_size + val_size) : (train_size + val_size + train_size)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GC9q7MyElAde","colab":{},"outputId":"3952dba5-0a1c-4240-b791-51b642efa42d"},"source":["print(\"Number of Daily Mail train data files is\", len(train_files))\n","print(\"Number of Daily Mail validation data files is\", len(val_files))\n","print(\"Number of Daily Mail test data files is\", len(test_files))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of Daily Mail train data files is 215006\n","Number of Daily Mail validation data files is 2000\n","Number of Daily Mail test data files is 2500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cVSYnheflAdh","colab":{},"outputId":"3bd43f98-205d-46ef-c9b6-cc3b954ddf98"},"source":["# To limit train data size\n","maxSize = 100000\n","\n","DM_data_directory = './dailymail_stories/'\n","total_train_data = writeData(train_files[0 : maxSize], DM_data_directory + 'DM_train.txt')\n","total_val_data = writeData(val_files, DM_data_directory + 'DM_valid_2000ex.txt')\n","total_test_data = writeData(test_files, DM_data_directory + 'DM_test_2500ex.txt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processed files so far is  1000\n","Processed files so far is  2000\n","Processed files so far is  3000\n","Processed files so far is  4000\n","Processed files so far is  5000\n","Processed files so far is  6000\n","Processed files so far is  7000\n","Processed files so far is  8000\n","Processed files so far is  9000\n","Processed files so far is  10000\n","Processed files so far is  11000\n","Processed files so far is  12000\n","Processed files so far is  13000\n","Processed files so far is  14000\n","Processed files so far is  15000\n","Processed files so far is  16000\n","Processed files so far is  17000\n","Processed files so far is  18000\n","Processed files so far is  19000\n","Processed files so far is  20000\n","Processed files so far is  21000\n","Processed files so far is  22000\n","Processed files so far is  23000\n","Processed files so far is  24000\n","Processed files so far is  25000\n","Processed files so far is  26000\n","Processed files so far is  27000\n","Processed files so far is  28000\n","Processed files so far is  29000\n","Processed files so far is  30000\n","Processed files so far is  31000\n","Processed files so far is  32000\n","Processed files so far is  33000\n","Processed files so far is  34000\n","Processed files so far is  35000\n","Processed files so far is  36000\n","Processed files so far is  37000\n","Processed files so far is  38000\n","Processed files so far is  39000\n","Processed files so far is  40000\n","Processed files so far is  41000\n","Processed files so far is  42000\n","Processed files so far is  43000\n","Processed files so far is  44000\n","Processed files so far is  45000\n","Processed files so far is  46000\n","Processed files so far is  47000\n","Processed files so far is  48000\n","Processed files so far is  49000\n","Processed files so far is  50000\n","Processed files so far is  51000\n","Processed files so far is  52000\n","Processed files so far is  53000\n","Processed files so far is  54000\n","Processed files so far is  55000\n","Processed files so far is  56000\n","Processed files so far is  57000\n","Processed files so far is  58000\n","Processed files so far is  59000\n","Processed files so far is  60000\n","Processed files so far is  61000\n","Processed files so far is  62000\n","Processed files so far is  63000\n","Processed files so far is  64000\n","Processed files so far is  65000\n","Processed files so far is  66000\n","Processed files so far is  67000\n","Processed files so far is  68000\n","Processed files so far is  69000\n","Processed files so far is  70000\n","Processed files so far is  71000\n","Processed files so far is  72000\n","Processed files so far is  73000\n","Processed files so far is  74000\n","Processed files so far is  75000\n","Processed files so far is  76000\n","Processed files so far is  77000\n","Processed files so far is  78000\n","Processed files so far is  79000\n","Processed files so far is  80000\n","Processed files so far is  81000\n","Processed files so far is  82000\n","Processed files so far is  83000\n","Processed files so far is  84000\n","Processed files so far is  85000\n","Processed files so far is  86000\n","Processed files so far is  87000\n","Processed files so far is  88000\n","Processed files so far is  89000\n","Processed files so far is  90000\n","Processed files so far is  91000\n","Processed files so far is  92000\n","Processed files so far is  93000\n","Processed files so far is  94000\n","Processed files so far is  95000\n","Processed files so far is  96000\n","Processed files so far is  97000\n","Processed files so far is  98000\n","Processed files so far is  99000\n","Processed files so far is  100000\n","Number of converted files is  88120\n","Processed files so far is  1000\n","Processed files so far is  2000\n","Number of converted files is  1753\n","Processed files so far is  1000\n","Processed files so far is  2000\n","Number of converted files is  2168\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"o0eyBcG2lAdm"},"source":["# Data Analysis"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9YYYBXrE-geM"},"source":["This section provides code to check the number of unique answers for each dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"meOtftRwlAdm","colab":{}},"source":["def collect_true_answers(data_file):\n","    \"\"\"\n","    This method reads data files and returns a dictionary with (true answer, its frequency) as \n","      key-value pairs\n","\n","    Parameters:\n","    'data_file': Directory to data file\n","    \"\"\"\n","    \n","    true_answers = {}\n","    counter = 0\n","  \n","    with gfile.GFile(data_file, mode=\"r\") as data_file:\n","        for line in data_file:\n","            counter += 1\n","\n","            if counter % 22 == 21:\n","                \"\"\"\n","                line.split(\"\\t\")[0] = query\n","                line.split(\"\\t\")[1] = true answer\n","                line.split(\"\\t\")[2] = empty string\n","                line.split(\"\\t\")[3] = candidate answers\n","                \"\"\"\n","                \n","                true_answers.update({line.split(\"\\t\")[1]: true_answers.get(line.split(\"\\t\")[1], 0) + 1})\n","                \n","    return true_answers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LVxzXr3klAdq","colab":{}},"source":["data_sets = [\"CNN\", \"DM\"]\n","\n","for i in range(len(data_sets)):\n","    \n","    # Train data\n","    data_directory = \"./data/\" + data_sets[i] + \"_data/\" + data_sets[i] + \"_train.txt\"\n","    dictionary = collect_true_answers(data_directory)\n","    \n","    print(\"Unique answers for \" + data_sets[i] + \" train data is:\", len(dictionary))\n","    print(\"total answers for \" + data_sets[i] + \" train data is:\", sum(dictionary.values()))\n","    \n","    # Validation data\n","    data_directory = \"./data/\" + data_sets[i] + \"_data/\" + data_sets[i] + \"_valid_2000ex.txt\"\n","    dictionary = collect_true_answers(data_directory)\n","    \n","    print(\"Unique answers for \" + data_sets[i] + \" validation data is:\", len(dictionary))\n","    print(\"total answers for \" + data_sets[i] + \" validation data is:\", sum(dictionary.values()))\n","\n","    # Test data\n","    data_directory = \"./data/\" + data_sets[i] + \"_data/\" + data_sets[i] + \"_test_2500ex.txt\"\n","    dictionary = collect_true_answers(data_directory)\n","    \n","    print(\"Unique answers for \" + data_sets[i] + \" test data is:\", len(dictionary))\n","    print(\"total answers for \" + data_sets[i] + \" test data is:\", sum(dictionary.values()))"],"execution_count":null,"outputs":[]}]}