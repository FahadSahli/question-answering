{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HAN_Model.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P5qAksFbe-ai"},"source":["# Imports"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cUwIXCYXfFXZ","colab":{},"executionInfo":{"status":"ok","timestamp":1594939223299,"user_tz":-180,"elapsed":2243,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}}},"source":["import random\n","import sys\n","import numpy as np\n","import tensorflow.compat.v1 as tf_compat_v1\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.compat.v1.nn.rnn_cell import GRUCell, MultiRNNCell\n","import os\n","from functools import reduce\n","from tensorflow.python.platform import gfile\n","import csv\n","import time\n","import pandas as pd"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zMlItPmE3rqG","colab":{},"executionInfo":{"status":"ok","timestamp":1594939223300,"user_tz":-180,"elapsed":2238,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}}},"source":["tf_compat_v1.disable_eager_execution()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"FNXGjOegyRjO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1594939250419,"user_tz":-180,"elapsed":20063,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}},"outputId":"89eb7d42-6cb7-4ac0-ec90-6f67a76130af"},"source":["#connection to google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G3G6rV3FyTYM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594939250424,"user_tz":-180,"elapsed":19800,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}}},"source":["path_to_drive = \"/content/drive/My Drive/192/Thesis/\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"hlhowXQTyYxF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594939319095,"user_tz":-180,"elapsed":847,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}}},"source":["# Directories to data\n","data_type = \"CN\"\n","data_dir = path_to_drive + \"data/\" + data_type + \"_data/\"\n","train_file = data_type + \"_train\"\n","valid_file = data_type + \"_valid_2000ex\"\n","test_file = data_type + \"_test_2500ex\"\n","\n","# Directory to store processed data\n","output_dir = \"processedData\"\n","\n","# Directory to pre-trained Glove\n","embedding_file = path_to_drive + \"embeddings/glove.6B.200d.txt\"\n","\n","# Number of words to keep\n","vocab_size = 100000\n","\n","# Model hyperparameters\n","hidden_size = 384\n","embedding_dim = 200\n","\n","batch_size = 64\n","num_epoches = 100\n","dropout_rate = 0.0\n","\n","two_encoding_layers=False\n","LM_embedding = False\n","embedding_type = \"First\"\n","\n","optimizer = \"ADAM\"\n","beta1 = 0.9\n","beta2 = 0.999\n","learning_rate = 0.001\n","grad_clipping = 10\n","\n","last_val_acc = 0.0\n","last_val_loss = sys.maxsize\n","\n","_EPSILON = 10e-8\n","EoS_ID = 2\n","\n","# Directory to check points\n","path_to_check_points = path_to_drive + 'check_points/'\n","path_to_developed_model = path_to_check_points + \"firstModel/\" + data_type + \"/\"\n","path_to_developed_model = path_to_developed_model + embedding_type + \"/\" if LM_embedding else path_to_developed_model\n","path_to_baseline_model = path_to_check_points + \"replicatedHAN/\" + data_type + \"/\"\n","\n","\n","# To be deleted\n","path_to_developed_model = path_to_developed_model + \"LR/\"\n","#path_to_baseline_model = path_to_baseline_model + \"experimenting/beta1/\"\n","\n","weight_path = path_to_baseline_model if two_encoding_layers else path_to_developed_model\n","\n","# To be removed\n","weight_path = './'\n","#Beta"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hYZjEACnfSmK"},"source":["# Global Parameters"]},{"cell_type":"markdown","metadata":{"id":"gBr-OJK7rwA0","colab_type":"text"},"source":["## Hyper-parameters"]},{"cell_type":"code","metadata":{"id":"4MTiHCZjrzqN","colab_type":"code","colab":{}},"source":["# Number of words to keep\n","vocab_size = 100000\n","\n","## Model hyperparameters:\n","\n","# Output space of GRU\n","hidden_size = 384\n","embedding_dim = 200\n","\n","batch_size = 64\n","num_epoches = 100\n","dropout_rate = 0.0\n","\n","# A Boolean to indicate the usage of 2 text enconding layers\n","two_encoding_layers=False\n","\n","# A Boolean to indicate the usage of Language Model (e.g., BERT) embeddings\n","LM_embedding = False\n","# BERT's layer from which embeddings are extracted\n","embedding_type = \"First\"\n","\n","optimizer = \"ADAM\"\n","beta1 = 0.9\n","beta2 = 0.999\n","learning_rate = 0.001\n","grad_clipping = 10\n","_EPSILON = 10e-8\n","\n","# Parameters used for early stopping, val = validation, acc = accuracy\n","last_val_acc = 0.0\n","last_val_loss = sys.maxsize\n","\n","# End of Sentence ID, it is not important\n","EoS_ID = 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xSGSlb1Lroqe","colab_type":"text"},"source":["## Paths to data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Vupwlt2ZfM0S","colab":{}},"source":["# Directory to data\n","data_type = \"CN\"\n","data_dir = \"./\" + \"data/\" + data_type + \"_data/\"\n","\n","# File names\n","train_file = data_type + \"_train\"\n","valid_file = data_type + \"_valid_2000ex\"\n","test_file = data_type + \"_test_2500ex\"\n","\n","# Directory to processed data (e.g., ID files)\n","output_dir = \"processedData\"\n","\n","# Directory to check points\n","weight_path = \"./\" + 'check_points/'\n","\n","# Directory to pre-trained Glove\n","embedding_file = \"./\" + \"embeddings/glove.6B.200d.txt\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"REXJWQ66ga1l"},"source":["# Read Data"]},{"cell_type":"markdown","metadata":{"id":"J3vpwzv5ucAe","colab_type":"text"},"source":["This section includes methods to read and prepare data."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"30teb23mgEpp","colab":{},"executionInfo":{"status":"ok","timestamp":1594939329464,"user_tz":-180,"elapsed":1090,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}}},"source":["def paths_to_processed_data(data_dir, train_file, valid_file, test_file, vocab_size, output_dir):\n","  \"\"\"\n","  This method takes paths to all data files and generates paths to corresponding ID files\n","  \"\"\"\n","\n","  idx_train_file = os.path.join(data_dir, output_dir, train_file + \".%d.id.txt\" % vocab_size)\n","  idx_valid_file = os.path.join(data_dir, output_dir, valid_file + \".%d.id.txt\" % vocab_size)\n","  idx_test_file = os.path.join(data_dir, output_dir, test_file + \".%d.id.txt\" % vocab_size)\n","  vocab_file = os.path.join(data_dir, output_dir, \"vocab.%d.txt\" % vocab_size)\n","\n","  return vocab_file, idx_train_file, idx_valid_file, idx_test_file\n","\n","def read_processed_cbt_data(file):\n","  \"\"\"\n","  This method Reads a data file\n","  \"\"\"\n","\n","  documents, questions, answers, candidates = [], [], [], []\n","    \n","  with gfile.GFile(file, mode=\"r\") as f:\n","    counter = 0\n","    d, q, a, A = [], [], [], []\n","    for line in f:\n","      counter += 1\n","      \n","      if counter % 100000 == 0:\n","        print(\"Reading line %d in %s\" % (counter, file))\n","\n","      if counter % 22 == 21:\n","        \n","        tmp = line.strip().split(\"\\t\")\n","        q = tmp[0].split(\" \") + [EoS_ID]\n","        a = [1 if tmp[1] == i else 0 for i in d]\n","\n","        A = [a for a in tmp[2].split(\"|\")]\n","\n","        # Put the correct answer first\n","        A.remove(tmp[1])\n","        A.insert(0, tmp[1])\n","\n","      elif counter % 22 == 0:\n","        documents.append(d)\n","        questions.append(q)\n","        answers.append(a)\n","        candidates.append(A)\n","        \n","        d, q, a, A = [], [], [], []\n","\n","      else:\n","        # Add EoS ID at the end of each sentence\n","        d_tem = [i for i in line.strip().split(\" \") if i != '']\n","        d.extend(d_tem + [EoS_ID])\n","\n","  d_lens = [len(i) for i in documents]\n","  q_lens = [len(i) for i in questions]\n","\n","  avg_d_len = reduce(lambda x, y: x + y, d_lens) / len(documents)\n","  print(\"Document average length: %d.\" % avg_d_len)\n","  print(\"Document midden length: %d.\" % len(sorted(documents, key=len)[len(documents) // 2]))\n","\n","  avg_q_len = reduce(lambda x, y: x + y, q_lens) / len(questions)\n","  print(\"Question average length: %d.\" % avg_q_len)\n","  print(\"Question midden length: %d.\" % len(sorted(questions, key=len)[len(questions) // 2]))\n","  return documents, questions, answers, candidates"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5EjMEusSlFnW","colab":{},"executionInfo":{"status":"ok","timestamp":1594939329465,"user_tz":-180,"elapsed":735,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}}},"source":["def load_vocab(vocab_file):\n","  \"\"\"\n","  This method loads 'vocab_file'. It returns a 'word_dict' with entries as {word : its ID}\n","\n","  Parameters:\n","    'vocab_file': Path to vocab file\n","  \"\"\"\n","\n","  if not gfile.Exists(vocab_file):\n","    raise ValueError(\"Vocabulary file %s not found.\", vocab_file)\n","\n","  word_dict = {}\n","  word_id = 0\n","\n","  with gfile.GFile(vocab_file, \"r\") as f:\n","    for line in f:\n","\n","      # Line has a single word with trailing new line char which needs to be removed\n","      word_dict.update({line.strip(): word_id})\n","      word_id += 1\n","\n","  return word_dict"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PST7oA0kjzWV","colab":{},"executionInfo":{"status":"ok","timestamp":1594939329913,"user_tz":-180,"elapsed":833,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}}},"source":["def gen_embeddings(word_dict, embed_dim, in_file=None, init=np.zeros):\n","    \"\"\"\n","    Create an initialized word vector matrix for the vocabulary. If a word is not in the word vector file, \n","      a vector will be initialized randomly.\n","    \n","    :param word_dict: Word to id mapping\n","    :param embed_dim: The dimensions of the word vector.\n","    :param in_file: Pre-trained word vector file. \n","    :param init: How to initialize words not found in the pre-training file\n","    :return: Word vector matrix\n","    \"\"\"\n","    num_words = max(word_dict.values()) + 1\n","    embedding_matrix = init(-0.1, 0.1, (num_words, embed_dim))\n","    print('Embeddings: %d x %d' % (num_words, embed_dim))\n","\n","    if not in_file:\n","        return embedding_matrix\n","\n","    assert get_dim(in_file) == embed_dim\n","    print('Loading embedding file: %s' % in_file)\n","\n","    pre_trained = 0\n","    for line in open(in_file):\n","        sp = line.split()\n","        if sp[0] in word_dict:\n","            pre_trained += 1\n","            embedding_matrix[word_dict[sp[0]]] = np.asarray([float(x) for x in sp[1:]], dtype=np.float32)\n","\n","    print('Pre-trained: %d (%.2f%%)' %\n","                 (pre_trained, pre_trained * 100.0 / num_words))\n","    return embedding_matrix\n","\n","\n","def get_dim(in_file):\n","    \"\"\"\n","    This method gets dimension of stored vectors\n","    \"\"\"\n","    line = gfile.GFile(in_file, mode='r').readline()\n","    return len(line.split()) - 1\n","\n","\n","def get_max_length(d_bt):\n","    lens = [len(i) for i in d_bt]\n","    return max(lens)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"upEivS8Bgu6L"},"source":["# Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"21T2Ln9mgdwL","colab":{},"executionInfo":{"status":"ok","timestamp":1594939332004,"user_tz":-180,"elapsed":2179,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}}},"source":["class HAN_model(object):\n","    def __init__(self, word_dict, embedding_matrix,\n","                 d_len, q_len, sess, embedding_dim,\n","                 hidden_size, weight_path,\n","                 use_lstm=False, two_encoding_layers=False):\n","      \n","        self.weight_path = weight_path\n","        self.word_dict = word_dict\n","        self.vocab_size = len(embedding_matrix)\n","        self.d_len = d_len\n","        self.q_len = q_len\n","        self.sess = sess\n","        self.two_encoding_layers = two_encoding_layers\n","\n","        self.A_len = 10\n","\n","        # Prepare embeddings\n","        with tf.device(\"/cpu:0\"):\n","            self.embedding = tf.Variable(initial_value=embedding_matrix, trainable=False,\n","                                    name=\"embedding_matrix_w\",\n","                                    dtype=\"float32\")\n","            \n","        print(\"Embedding matrix shape:%d x %d\" % (len(embedding_matrix), embedding_dim))\n","        \n","        self.text_encoder = GRUCell(num_units=hidden_size)\n","\n","        # Model input and output\n","        self.q_input = tf_compat_v1.placeholder(dtype=tf.int32, shape=(None, self.q_len), name=\"q_input\")\n","        self.d_input = tf_compat_v1.placeholder(dtype=tf.int32, shape=(None, self.d_len), name=\"d_input\")\n","        self.context_mask_bt = tf_compat_v1.placeholder(dtype=tf.float32, shape=(None, self.d_len), name=\"context_mask_bt\")\n","        self.candidates_bi = tf_compat_v1.placeholder(dtype=tf.int32, shape=(None, self.A_len), name=\"candidates_bi\")\n","        self.y_true = tf_compat_v1.placeholder(shape=(None, self.A_len), dtype=tf.float32, name=\"y_true\")\n","\n","\n","    def att_dot(self, x):\n","        \"\"\"Attention point multiplication function\"\"\"\n","        d_btf, q_bf = x\n","        \n","        # 'res' shape = (None, 1, max_d_length)\n","        res = K.batch_dot(tf.expand_dims(q_bf, -1), d_btf, (1, 2))\n","        return tf.reshape(res, [-1, self.d_len])\n","\n","    # Attention-sum process\n","    def sum_prob_of_word(self, word_ix, sentence_ixs, sentence_attention_probs):\n","        word_ixs_in_sentence = tf.where(tf.equal(sentence_ixs, word_ix))\n","        return tf.reduce_sum(tf.gather(sentence_attention_probs, word_ixs_in_sentence))\n","\n","    # noinspection PyUnusedLocal\n","    def sum_probs_single_sentence(self, prev, cur):\n","        candidate_indices_i, sentence_ixs_t, sentence_attention_probs_t = cur\n","        result = tf.scan(\n","            fn=lambda previous, x: self.sum_prob_of_word(x, sentence_ixs_t, sentence_attention_probs_t),\n","            elems=[candidate_indices_i],\n","            initializer=tf.constant(0., dtype=\"float32\"))\n","        return result\n","\n","    def sum_probs_batch(self, candidate_indices_bi, sentence_ixs_bt, sentence_attention_probs_bt):\n","        result = tf.scan(\n","            fn=self.sum_probs_single_sentence,\n","            elems=[candidate_indices_bi, sentence_ixs_bt, sentence_attention_probs_bt],\n","            initializer=tf.Variable([0] * self.A_len, dtype=\"float32\"))\n","        return result\n","\n","    def build_model(self):\n","        \n","        \"\"\"\n","        'y = sign(x) = -1 if x < 0; 0 if x == 0; 1 if x > 0.'\n","        'tf.abs' = absolute\n","        \"\"\"\n","        d_lens = tf.reduce_sum(tf.sign(tf.abs(self.d_input)), 1)\n","        q_lens = tf.reduce_sum(tf.sign(tf.abs(self.q_input)), 1)\n","\n","        # Query encoder\n","        with tf_compat_v1.variable_scope('q_encoder', initializer=tf.initializers.GlorotUniform(), reuse=tf_compat_v1.AUTO_REUSE):\n","\n","            # output shape: (None, max_q_length, embedding_dim), max_q_length = max query length\n","            q_embed = tf.nn.embedding_lookup(self.embedding, self.q_input)\n","\n","            q_cell = MultiRNNCell(cells=[self.text_encoder] * 1)\n","\n","            \"\"\"            \n","            Takes input and builds independent forward and backward RNNs. The input_size of forward and backward cell must match. \n","              The initial state for both directions is zero by default (but can be set optionally) and no intermediate states are ever returned -- \n","                the network is fully unrolled for the given (passed in) length(s) of the sequence(s) or completely unrolled if length(s) is not given.\n","\n","            Outputs:\n","            A tuple (outputs, output_states) where: outputs: A tuple (output_fw, output_bw), and output_states: A tuple (output_state_fw, output_state_bw) \n","              containing the forward and the backward final states of bidirectional rnn.\n","\n","            \"\"\"\n","            outputs, last_states = tf_compat_v1.nn.bidirectional_dynamic_rnn(cell_bw=q_cell,\n","                                                                   cell_fw=q_cell,\n","                                                                   dtype=\"float32\",\n","                                                                   sequence_length=q_lens,\n","                                                                   inputs=q_embed,\n","                                                                   swap_memory=True)\n","            # 'q_encode' output shape: (None, hidden_size * 2)\n","            \"\"\"\n","            Concatenate along the last axis so that enties at 'last_states[0]' \n","              gets concatenated with the corresponding entries at 'last_states[0]'\n","              see https://www.tensorflow.org/api_docs/python/tf/concat\n","            \"\"\"\n","            self.q_encode = tf.concat([last_states[0][-1], last_states[1][-1]], axis=-1)\n","\n","        # Level 1 (L1) document encoder\n","        with tf_compat_v1.variable_scope('d_encoder_L1', initializer=tf.initializers.GlorotUniform(), reuse=tf_compat_v1.AUTO_REUSE):\n","\n","            # output shape: (None, max_d_length, embedding_dim),  max_d_length = max document length\n","            d_embed_L1 = tf.nn.embedding_lookup(self.embedding, self.d_input)\n","\n","            d_cell_L1 = MultiRNNCell(cells=[self.text_encoder] * 1)\n","            outputs_L1, last_states_L1 = tf_compat_v1.nn.bidirectional_dynamic_rnn(cell_bw=d_cell_L1,\n","                                                                   cell_fw=d_cell_L1,\n","                                                                   dtype=\"float32\",\n","                                                                   sequence_length=d_lens,\n","                                                                   inputs=d_embed_L1,\n","                                                                   swap_memory=True)\n","            \n","            # 'd_encode_L1' output shape: (None, max_d_length, hidden_size * 2)\n","            self.d_encode_L1 = tf.concat(outputs_L1, axis=-1)\n","\n","        # Level 1 (L1) attention\n","        with tf_compat_v1.variable_scope('attention_L1', reuse=tf_compat_v1.AUTO_REUSE):\n","\n","            # Output shape = (None, max_d_length)\n","            self.attention_L1 = self.att_dot([self.d_encode_L1, self.q_encode])\n","            self.attention_softmax_L1 = tf.nn.softmax(logits=self.attention_L1, name=\"attention_softmax_L1\")\n","\n","            # Compute attented document\n","            # Shape = (None, max_d_length, hidden_size * 2)\n","            self.attented_doc = tf.multiply(tf.expand_dims(self.attention_softmax_L1, -1), self.d_encode_L1, name=\"attented_doc\")\n","\n","        if(self.two_encoding_layers):\n","            # Level 2 (L2) document encoder\n","            with tf_compat_v1.variable_scope('d_encoder_L2', initializer=tf.initializers.GlorotUniform(), reuse=tf_compat_v1.AUTO_REUSE):\n","\n","                d_cell_L2 = MultiRNNCell(cells=[self.text_encoder] * 1)\n","                outputs_L2, last_states_L2 = tf_compat_v1.nn.bidirectional_dynamic_rnn(cell_bw=d_cell_L2,\n","                                                                      cell_fw=d_cell_L2,\n","                                                                      dtype=\"float32\",\n","                                                                      sequence_length=d_lens,\n","                                                                      inputs=self.attented_doc,\n","                                                                      swap_memory=True)\n","                \n","                # 'd_encode_L2' output shape: (None, max_d_length, hidden_size * 2)\n","                self.d_encode_L2 = tf.concat(outputs_L2, axis=-1)\n","\n","        # Level 2 (L2) attention\n","        with tf_compat_v1.variable_scope('attention_L2', reuse=tf_compat_v1.AUTO_REUSE):\n","          \n","            # Output shape = (None, max_d_length)\n","            self.attention_L2 = self.att_dot([self.d_encode_L2, self.q_encode]) if self.two_encoding_layers else self.att_dot([self.attented_doc, self.q_encode])\n","            \n","            self.attention_softmax_L2 = tf.nn.softmax(logits=self.attention_L2, name=\"softmax_attention_L2\")\n","\n","            # Output shape = (None, max_d_length)\n","            self.last_prob = tf.multiply(self.attention_softmax_L2, self.attention_softmax_L1, name=\"last_prob\")\n","        \n","\n","        # Attention summation\n","        # Output shape = (None, i) where i = max_candidate_length = 10\n","        self.y_hat = self.sum_probs_batch(self.candidates_bi, self.d_input, self.last_prob)\n","\n","        # Cross entropy loss function\n","        output = self.y_hat / tf.reduce_sum(self.y_hat,\n","                                            axis=len(self.y_hat.get_shape()) - 1,\n","                                            keepdims=True)\n","        \n","        # Compute crossentropy\n","        epsilon = tf.convert_to_tensor(_EPSILON, output.dtype.base_dtype, name=\"epsilon\")\n","        output = tf.clip_by_value(output, epsilon, 1. - epsilon)\n","        self.loss = tf.reduce_mean(- tf.reduce_sum(self.y_true * tf.math.log(output),\n","                                                   axis=len(output.get_shape()) - 1))\n","        \n","        # Calculate accuracy\n","        self.correct_prediction = tf.reduce_sum(tf.sign(tf.cast(tf.equal(tf.argmax(self.y_hat, 1),\n","                                                                         tf.argmax(self.y_true, 1)), \"float\")))\n","        # Model serialization tool\n","        self.saver = tf_compat_v1.train.Saver()\n","\n","\n","    def train(self, train_data, valid_data, batch_size, epochs, opt_name, lr, beta1, beta2, grad_clip):\n","        \"\"\"\n","        Model training\n","        \"\"\"\n","        # Preprocessing the input\n","        questions_ok, documents_ok, context_mask, candidates_ok, y_true = self.preprocess_input_sequences(train_data)\n","        v_questions, v_documents, v_context_mask, v_candidates, v_y_true = self.preprocess_input_sequences(valid_data)\n","\n","        # Define the optimization method of the model\n","        if opt_name == \"SGD\":\n","            optimizer = tf_compat_v1.train.GradientDescentOptimizer(learning_rate=lr)\n","        elif opt_name == \"ADAM\":\n","            optimizer = tf_compat_v1.train.AdamOptimizer(learning_rate=lr, beta1=beta1, beta2=beta2)\n","        else:\n","            raise NotImplementedError(\"Other Optimizer Not Implemented.-_-||\")\n","\n","        # Gradient cropping\n","        grad_vars = optimizer.compute_gradients(self.loss)\n","        grad_vars = [\n","            (tf.clip_by_norm(grad, grad_clip), var)\n","            if grad is not None else (grad, var)\n","            for grad, var in grad_vars]\n","        train_op = optimizer.apply_gradients(grad_vars)\n","        self.sess.run(tf_compat_v1.global_variables_initializer())\n","\n","        # Load a previously trained model\n","        self.load_weight()\n","\n","        # Prepare validation set data\n","        v_data = {self.q_input: v_questions,\n","                  self.d_input: v_documents,\n","                  self.context_mask_bt: v_context_mask,\n","                  self.candidates_bi: v_candidates,\n","                  self.y_true: v_y_true}\n","\n","        # early stopping parameter\n","        best_val_loss, best_val_acc, patience, lose_times = last_val_loss, last_val_acc, 5, 0\n","\n","        # Start training\n","        corrects_in_epoch, loss_in_epoch = 0, 0\n","        batch_num, v_batch_num = len(questions_ok) // batch_size, len(v_questions) // batch_size\n","        batch_idx, v_batch_idx = np.random.permutation(batch_num), np.arange(v_batch_num)\n","        print(\"Train on {} batches, {} samples per batch.\".format(batch_num, batch_size))\n","        print(\"Validate on {} batches, {} samples per batch.\".format(v_batch_num, batch_size))\n","\n","        for step in range(batch_num * epochs):\n","            # End of an Epoch, output log and shuffle\n","            if step % batch_num == 0:\n","                corrects_in_epoch, loss_in_epoch = 0, 0\n","                print(\"--------Epoch : {}\".format(step // batch_num + 1))\n","                np.random.shuffle(batch_idx)\n","\n","            # Get the data for the next batch\n","            _slice = np.index_exp[\n","                     batch_idx[step % batch_num] * batch_size:(batch_idx[step % batch_num] + 1) * batch_size]\n","            data = {self.q_input: questions_ok[_slice],\n","                    self.d_input: documents_ok[_slice],\n","                    self.context_mask_bt: context_mask[_slice],\n","                    self.candidates_bi: candidates_ok[_slice],\n","                    self.y_true: y_true[_slice]}\n","\n","            # Train, update parameters, output current accuracy of Epoch\n","            start_time = time.time()\n","            loss_, _, corrects_in_batch = self.sess.run([self.loss, train_op, self.correct_prediction],\n","                                                        feed_dict=data)\n","            \n","            corrects_in_epoch += corrects_in_batch\n","            loss_in_epoch += loss_ * batch_size\n","            nums_in_epoch = (step % batch_num + 1) * batch_size\n","            print(\"Trained samples in this epoch : {}\".format(nums_in_epoch))\n","            print(\"Step : {}/{}.\\nLoss : {:.4f}.\\nAccuracy : {:.4f}\".format(step % batch_num,\n","                                                                                   batch_num,\n","                                                                                   loss_in_epoch / nums_in_epoch,\n","                                                                                   corrects_in_epoch / nums_in_epoch))\n","\n","            # Save the model every 200 steps and use the validation set to calculate the accuracy rate and determine whether it is early stop\n","            if step % 200 == 0 and step != 0:\n","                # Due to insufficient GPU memory, it is still calculated as batch\n","                val_num, val_corrects, v_loss = 0, 0, 0\n","                for i in range(v_batch_num):\n","                    start = v_batch_idx[i % v_batch_num] * batch_size\n","                    stop = (v_batch_idx[i % v_batch_num] + 1) * batch_size\n","                    _v_slice = np.index_exp[start:stop]\n","                    v_data = {self.q_input: v_questions[_v_slice],\n","                              self.d_input: v_documents[_v_slice],\n","                              self.context_mask_bt: v_context_mask[_v_slice],\n","                              self.candidates_bi: v_candidates[_v_slice],\n","                              self.y_true: v_y_true[_v_slice]}\n","                    loss_, v_correct = self.sess.run([self.loss, self.correct_prediction], feed_dict=v_data)\n","                    val_num = val_num + batch_size\n","                    val_corrects = val_corrects + v_correct\n","                    v_loss = v_loss + loss_ * batch_size\n","\n","                val_acc = val_corrects / val_num\n","                val_loss = v_loss / val_num\n","                print(\"Val acc : {:.4f}\".format(val_acc))\n","                print(\"Val Loss : {:.4f}\".format(val_loss))\n","\n","                if val_acc > best_val_acc or val_loss < best_val_loss:\n","                    # Save a better model\n","                    lose_times = 0\n","                    \n","                    best_val_loss = val_loss if val_loss < best_val_loss else best_val_loss\n","                    best_val_acc = val_acc if val_acc > best_val_acc else best_val_acc\n","                    path = self.saver.save(self.sess,\n","                                           self.weight_path + \\\n","                                           'machine_reading-val_acc-{:.4f}-val_loss-{:.4f}.model'.format(val_acc, val_loss),\n","                                           global_step=step)\n","                    print(\"Save model to {}.\".format(path))\n","\n","                else:\n","                    lose_times += 1\n","                    print(\"Lose_time/Patience : {}/{} .\".format(lose_times, patience))\n","                    if lose_times >= patience:\n","                        print(\"Oh u, stop training.\".format(lose_times, patience))\n","                        exit(0)\n","\n","    def test(self, test_data, batch_size):\n","        \n","        questions_ok, documents_ok, context_mask, candidates_ok, y_true = self.preprocess_input_sequences(test_data)\n","        print(\"Test on {} samples, {} per batch.\".format(len(questions_ok), batch_size))\n","\n","        # Load a previously trained model\n","        self.load_weight()\n","        \n","        # Accuarcy\n","        Accuracies = []\n","        delays = []\n","        \n","        # Testing\n","        batch_num = len(questions_ok) // batch_size\n","        batch_idx = np.arange(batch_num)\n","        correct_num, total_num = 0, 0\n","        for i in range(batch_num):\n","            start = batch_idx[i % batch_num] * batch_size\n","            stop = (batch_idx[i % batch_num] + 1) * batch_size\n","            _slice = np.index_exp[start:stop]\n","            data = {self.q_input: questions_ok[_slice],\n","                    self.d_input: documents_ok[_slice],\n","                    self.context_mask_bt: context_mask[_slice],\n","                    self.candidates_bi: candidates_ok[_slice],\n","                    self.y_true: y_true[_slice]}\n","            \n","            start = time.time()  \n","            correct, = self.sess.run([self.correct_prediction], feed_dict=data)\n","            end = time.time() - start\n","            delays.append(end / batch_size)\n","            \n","            correct_num, total_num = correct_num + correct, total_num + batch_size\n","            Accuracies.append(correct / batch_size)\n","            \n","        test_acc = correct_num / total_num\n","        print(\"Test accuracy is : {:.5f}\".format(test_acc))\n","        \n","        return Accuracies, test_acc, delays\n","\n","    def load_weight(self):\n","\n","        ckpt = tf.train.get_checkpoint_state(self.weight_path)\n","        if ckpt is not None:\n","            print(\"Load model from {}.\".format(ckpt.model_checkpoint_path))\n","            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n","        else:\n","            print(\"No previous models.\")\n","\n","    @staticmethod\n","    def union_shuffle(data):\n","        d, q, a, A = data\n","        c = list(zip(d, q, a, A))\n","        random.shuffle(c)\n","        return zip(*c)\n","\n","    def preprocess_input_sequences(self, data, shuffle=True):\n","        \"\"\"\n","        Preprocessing input as：\n","          shuffle\n","          PAD To a fixed-length sequence\n","          y_true is a vector of length self.A_len, index = 0 is the correct answer, and one-hot encoding\n","        \"\"\"\n","        documents, questions, answer, candidates = self.union_shuffle(data) if shuffle else data\n","        d_lens = [len(i) for i in documents]\n","\n","        questions_ok = pad_sequences(questions, maxlen=self.q_len, dtype=\"int32\", padding=\"post\", truncating=\"post\")\n","        documents_ok = pad_sequences(documents, maxlen=self.d_len, dtype=\"int32\", padding=\"post\", truncating=\"post\")\n","        context_mask = K.eval(tf.sequence_mask(d_lens, self.d_len, dtype=tf.float32))\n","        candidates_ok = pad_sequences(candidates, maxlen=self.A_len, dtype=\"int32\", padding=\"post\", truncating=\"post\")\n","        y_true = np.zeros_like(candidates_ok)\n","        y_true[:, 0] = 1\n","        return questions_ok, documents_ok, context_mask, candidates_ok, y_true"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VKjPrxVegeqi"},"source":["# Run"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LV-ZUpuAtTZa"},"source":["## Data Preparation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"erh8Do9DjEjt","colab":{},"executionInfo":{"status":"ok","timestamp":1594939332005,"user_tz":-180,"elapsed":1218,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}}},"source":["vocab_file, idx_train_file, idx_valid_file, idx_test_file = paths_to_processed_data(\n","        data_dir, train_file, valid_file, test_file, vocab_size, output_dir)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1ro2FlVTtRyi","colab":{"base_uri":"https://localhost:8080/","height":663},"executionInfo":{"status":"ok","timestamp":1594939360481,"user_tz":-180,"elapsed":29337,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}},"outputId":"b208a02d-b214-4526-be26-7b94e2cf5525"},"source":["t_documents, t_questions, t_answer, t_candidates = read_processed_cbt_data(idx_train_file)\n","v_documents, v_questions, v_answers, v_candidates = read_processed_cbt_data(idx_valid_file)\n","test_documents, test_questions, test_answers, test_candidates = read_processed_cbt_data(idx_test_file)\n","\n","d_len = get_max_length(t_documents)\n","q_len = get_max_length(t_questions)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Reading line 100000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 200000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 300000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 400000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 500000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 600000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 700000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 800000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 900000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 1000000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 1100000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 1200000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 1300000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 1400000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 1500000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 1600000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 1700000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 1800000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 1900000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 2000000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 2100000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 2200000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 2300000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 2400000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 2500000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Reading line 2600000 in /content/drive/My Drive/192/Thesis/data/CN_data/processedData/CN_train.100000.id.txt\n","Document average length: 489.\n","Document midden length: 472.\n","Question average length: 31.\n","Question midden length: 28.\n","Document average length: 467.\n","Document midden length: 460.\n","Question average length: 29.\n","Question midden length: 27.\n","Document average length: 480.\n","Document midden length: 476.\n","Question average length: 31.\n","Question midden length: 28.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KnSPJd8Fx6NG","colab_type":"text"},"source":["## Get embeddings"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"m-UuwSQYPFzz","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1594939370298,"user_tz":-180,"elapsed":38187,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}},"outputId":"c13c0010-0426-4dbe-dadf-be0a381c969b"},"source":["# Initialize the word vector matrix, using a random uniform distribution in the interval (-0.1,0.1)\n","word_dict = load_vocab(vocab_file)\n","embedding_matrix = gen_embeddings(word_dict,\n","                                              embedding_dim,\n","                                              embedding_file,\n","                                              init=np.random.uniform)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Embeddings: 52909 x 200\n","Loading embedding file: /content/drive/My Drive/192/Thesis/embeddings/glove.6B.200d.txt\n","Pre-trained: 36345 (68.69%)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lQE_0ATOG7DE","colab_type":"text"},"source":["## Define Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"44FSkjtcPXyg","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1594939378700,"user_tz":-180,"elapsed":45383,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}},"outputId":"15aa5ad8-0671-4052-b9f3-d06f385a8cdb"},"source":["sess = tf_compat_v1.Session()\n","model = HAN_model(word_dict, embedding_matrix, d_len, q_len, sess,\n","                              embedding_dim, hidden_size,\n","                              weight_path, two_encoding_layers)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Embedding matrix shape:52909 x 200\n","WARNING:tensorflow:From <ipython-input-9-8b0712371b1d>:25: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1xbmjhTU8F4Q","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"ok","timestamp":1594939378993,"user_tz":-180,"elapsed":45098,"user":{"displayName":"Fahd Al Sahali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgkaYWUC0MfH4_gpXliKkJLZ3dwsiHm7Ok3qIYajw=s64","userId":"14743331008726039331"}},"outputId":"595ebcfc-78a4-4314-9b8a-acca4ee29218"},"source":["model.build_model()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-9-8b0712371b1d>:79: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-9-8b0712371b1d>:96: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:564: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:570: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:580: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eKmjnqixG7DI","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"M7TTd4_Kl5Di","colab":{}},"source":["model.train(train_data=(t_documents, t_questions, t_answer, t_candidates),\n","                    valid_data=(v_documents, v_questions, v_answers, v_candidates),\n","                    batch_size=batch_size,\n","                    epochs=num_epoches,\n","                    opt_name=optimizer,\n","                    lr=learning_rate,\n","                    beta1=beta1,\n","                    beta2=beta2,\n","                    grad_clip=grad_clipping)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZiSTTLQKG7DU","colab_type":"text"},"source":["## Testing"]},{"cell_type":"code","metadata":{"id":"_IolXn39G7DW","colab_type":"code","colab":{}},"source":["start_t = time.time()\n","batch_accuracies, test_accuracy, batch_delays = model.test((test_documents, test_questions, test_answers, test_candidates), batch_size)\n","start_t = time.time() - start_t\n","start_t"],"execution_count":null,"outputs":[]}]}